{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fyUeE2wWo1Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfFYIuDwQX0V",
        "outputId": "62ce0dee-e45d-4a2c-9ad6-c56a6b3d5cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 19 11:20:35 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# starting gpu\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "id": "DgUoQYe9RlWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip uninstall -y transformers accelerate\n",
        "!pip install transformers accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uhOU9m1Tp73",
        "outputId": "1eb59f9b-ce89-4f1d-9f20-9e15ea969e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2026.1.4)\n",
            "Found existing installation: transformers 4.57.6\n",
            "Uninstalling transformers-4.57.6:\n",
            "  Successfully uninstalled transformers-4.57.6\n",
            "Found existing installation: accelerate 1.12.0\n",
            "Uninstalling accelerate-1.12.0:\n",
            "  Successfully uninstalled accelerate-1.12.0\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Using cached transformers-4.57.6-py3-none-any.whl (12.0 MB)\n",
            "Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Installing collected packages: transformers, accelerate\n",
            "Successfully installed accelerate-1.12.0 transformers-4.57.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk    # loading dataset after loading it from the disk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# Autotokenizer : Text --> token.\n",
        "# AutoModelForSeq2SeqLM --> pipeline creation\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")\n"
      ],
      "metadata": {
        "id": "KYz4aGH-We85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba199e5-219d-4557-c8d0-cbd75560c7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, PegasusForConditionalGeneration, AutoModelForSeq2SeqLM\n",
        "\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly\")\n",
        "\n",
        "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")    # converting the words into token\n",
        "\n",
        "\n",
        "summary_ids = model.generate(inputs[\"input_ids\"])  # generating ids for the tokens\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]   # skipping the special tokens like comma, spaces etc, converting to text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "RhqHAnr5cClN",
        "outputId": "90e3d388-e899-4f91-dd45-484140ed2594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"California's largest electricity provider has said it will cut power to more than 300,000 customers for up to a week.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdyvpaywobIo",
        "outputId": "a45d481c-df10-459b-acaa-333e358ed99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,  1310,   131,   116,  1368,  3769,  2089,   148,   243,   126,\n",
              "           138,   999,   484,   112,   154,   197, 23963,   527,   118,   164,\n",
              "           112,   114,   396,   107,     1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfd4EJzHpJFi",
        "outputId": "6eef6c8f-6dab-4ce8-ff30-3bc2d5a45e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[14887,   759,  1005,  3163,   126,  2798,   109, 25690,   116,   115,\n",
              "          1407,   112, 13378,   118,   281,  7213, 10754,  1514,  1047,   107,\n",
              "           139,  2560,   117,   112,  1329,   109,   887,   113, 39471,   107,\n",
              "         16502,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40X18rjFpL-D",
        "outputId": "f4015a2a-0169-45cd-c96d-e8225142006a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PegasusTokenizerFast(name_or_path='google/pegasus-xsum', vocab_size=96103, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"<mask_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"<mask_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"<unk_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t5: AddedToken(\"<unk_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t6: AddedToken(\"<unk_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t7: AddedToken(\"<unk_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t8: AddedToken(\"<unk_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t9: AddedToken(\"<unk_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t10: AddedToken(\"<unk_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t11: AddedToken(\"<unk_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t12: AddedToken(\"<unk_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t13: AddedToken(\"<unk_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t14: AddedToken(\"<unk_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t15: AddedToken(\"<unk_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t16: AddedToken(\"<unk_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t17: AddedToken(\"<unk_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t18: AddedToken(\"<unk_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t19: AddedToken(\"<unk_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t20: AddedToken(\"<unk_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t21: AddedToken(\"<unk_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t22: AddedToken(\"<unk_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t23: AddedToken(\"<unk_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t24: AddedToken(\"<unk_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t25: AddedToken(\"<unk_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t26: AddedToken(\"<unk_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t27: AddedToken(\"<unk_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t28: AddedToken(\"<unk_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t29: AddedToken(\"<unk_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t30: AddedToken(\"<unk_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t31: AddedToken(\"<unk_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t32: AddedToken(\"<unk_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t33: AddedToken(\"<unk_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t34: AddedToken(\"<unk_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t35: AddedToken(\"<unk_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t36: AddedToken(\"<unk_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t37: AddedToken(\"<unk_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t38: AddedToken(\"<unk_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t39: AddedToken(\"<unk_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t40: AddedToken(\"<unk_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t41: AddedToken(\"<unk_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t42: AddedToken(\"<unk_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t43: AddedToken(\"<unk_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t44: AddedToken(\"<unk_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t45: AddedToken(\"<unk_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t46: AddedToken(\"<unk_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t47: AddedToken(\"<unk_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t48: AddedToken(\"<unk_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t49: AddedToken(\"<unk_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t50: AddedToken(\"<unk_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t51: AddedToken(\"<unk_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t52: AddedToken(\"<unk_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t53: AddedToken(\"<unk_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t54: AddedToken(\"<unk_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t55: AddedToken(\"<unk_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t56: AddedToken(\"<unk_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t57: AddedToken(\"<unk_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t58: AddedToken(\"<unk_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t59: AddedToken(\"<unk_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t60: AddedToken(\"<unk_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t61: AddedToken(\"<unk_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t62: AddedToken(\"<unk_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t63: AddedToken(\"<unk_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t64: AddedToken(\"<unk_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t65: AddedToken(\"<unk_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t66: AddedToken(\"<unk_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t67: AddedToken(\"<unk_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t68: AddedToken(\"<unk_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t69: AddedToken(\"<unk_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t70: AddedToken(\"<unk_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t71: AddedToken(\"<unk_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t72: AddedToken(\"<unk_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t73: AddedToken(\"<unk_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t74: AddedToken(\"<unk_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t75: AddedToken(\"<unk_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t76: AddedToken(\"<unk_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t77: AddedToken(\"<unk_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t78: AddedToken(\"<unk_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t79: AddedToken(\"<unk_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t80: AddedToken(\"<unk_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t81: AddedToken(\"<unk_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t82: AddedToken(\"<unk_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t83: AddedToken(\"<unk_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t84: AddedToken(\"<unk_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t85: AddedToken(\"<unk_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t86: AddedToken(\"<unk_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t87: AddedToken(\"<unk_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t88: AddedToken(\"<unk_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t89: AddedToken(\"<unk_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t90: AddedToken(\"<unk_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t91: AddedToken(\"<unk_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t92: AddedToken(\"<unk_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t93: AddedToken(\"<unk_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t94: AddedToken(\"<unk_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t95: AddedToken(\"<unk_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t96: AddedToken(\"<unk_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t97: AddedToken(\"<unk_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t98: AddedToken(\"<unk_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t99: AddedToken(\"<unk_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"<unk_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"<unk_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"<unk_100>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"<unk_101>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t104: AddedToken(\"<unk_102>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t105: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cuda\""
      ],
      "metadata": {
        "id": "hVYbfVuDpQwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "B4RlmoMCpvRF",
        "outputId": "2bd0e78c-7e5b-491a-da0c-c781063bfa3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning LLM\n",
        "\n",
        "- Every LLM has two components\n",
        "  - Tokenizer\n",
        "  - LLM itself\n",
        "\n",
        "  Why do they exist?\n",
        "  - Each LLM has their own tokenizer on which they can easily work with and produce meaningful context, else LLM would be total confused about the tokens embeded. The tokenizer controls the size of tokens, this shall be similar to how the LLMs were trained on."
      ],
      "metadata": {
        "id": "VB6lJnkOp9Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)  # load tokenizer\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)  # load model cnn_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-5yzwSpp3Qw",
        "outputId": "1a8b0189-8a44-494e-f4a4-ff029b170d59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the data\n",
        "\n",
        "!wget https://github.com/krishnaik06/datasets/raw/refs/heads/main/summarizer-data.zip\n",
        "!unzip summarizer-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rh5mR1aqqHi",
        "outputId": "547803b2-468c-40b0-bc3d-df090696e5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-19 11:25:01--  https://github.com/krishnaik06/datasets/raw/refs/heads/main/summarizer-data.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/krishnaik06/datasets/refs/heads/main/summarizer-data.zip [following]\n",
            "--2026-01-19 11:25:02--  https://raw.githubusercontent.com/krishnaik06/datasets/refs/heads/main/summarizer-data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7903594 (7.5M) [application/zip]\n",
            "Saving to: ‘summarizer-data.zip.1’\n",
            "\n",
            "summarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2026-01-19 11:25:02 (91.7 MB/s) - ‘summarizer-data.zip.1’ saved [7903594/7903594]\n",
            "\n",
            "Archive:  summarizer-data.zip\n",
            "replace samsum-test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to load the .arrow files or json  --> datadict\n",
        "dataset_samsum = load_from_disk('samsum_dataset')\n",
        "dataset_samsum"
      ],
      "metadata": {
        "id": "yR3u7SozrGP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset_samsum:\n",
        "  print(f\"{split}: {len(dataset_samsum[split])}\")"
      ],
      "metadata": {
        "id": "je6vbzOCrogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_samsum[\"test\"][1])"
      ],
      "metadata": {
        "id": "DQqoA06IuPAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_samsum[\"test\"][1]['dialogue'])"
      ],
      "metadata": {
        "id": "C5ovM8-dtNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_samsum[\"test\"][1]['summary'])"
      ],
      "metadata": {
        "id": "WoModudmt4gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing for fine tuning\n",
        "\n",
        "## Creating function to add inputs_ids, attention mask and labels --> as they are very much essential for re-training or finetuning\n",
        "\n",
        "def convert_examples_to_features(example_batch):\n",
        "  input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
        "\n",
        "  return {\n",
        "      'input_ids': input_encodings['input_ids'],\n",
        "      'attention_mask': input_encodings['attention_mask'],\n",
        "      'labels': target_encodings['input_ids']\n",
        "  }"
      ],
      "metadata": {
        "id": "EIRkUkbit_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ],
      "metadata": {
        "id": "88dgRVLPwSDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt # we can notice new columns added such as input_ids, attention_mask, lablels"
      ],
      "metadata": {
        "id": "6CDMoz7ewhMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum_pt[\"train\"][1]"
      ],
      "metadata": {
        "id": "AN_2prA4wjMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## need data-collator for sequence to sequence model\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ],
      "metadata": {
        "id": "SMEE4n-7wv-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum',\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ],
      "metadata": {
        "id": "85tq75gYo3we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model = model_pegasus,\n",
        "                  args = trainer_args,\n",
        "                  tokenizer = tokenizer,\n",
        "                  data_collator = seq2seq_data_collator,\n",
        "                  train_dataset = dataset_samsum_pt[\"test\"],      # since the train dataset is large in size\n",
        "                  eval_dataset = dataset_samsum_pt[\"validation\"]\n",
        "                  )"
      ],
      "metadata": {
        "id": "E1AtQAtr01Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZYOyKIsn1URw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation of finetuned model\n",
        "\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "\n",
        "  \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "  Yield successive batch-sized chunks from list_of_elements\"\"\"\n",
        "\n",
        "  for i in range(0, len(list_of_elements), batch_size):\n",
        "    yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset,\n",
        "                                metric,\n",
        "                                model,\n",
        "                                tokenizer,\n",
        "                                column_text = \"article\",\n",
        "                                column_summary = \"highlights\"):\n",
        "  article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "  target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "  for article_batch, target_batch in tqdm(\n",
        "      zip(article_batches, target_batches),\n",
        "      total = len(article_batches)):\n",
        "\n",
        "      inputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "      summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                                 attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                                 length_penalty=0.8,\n",
        "                                 num_beams=8,\n",
        "                                 max_length=128)\n",
        "      ''' Parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "      decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n",
        "\n",
        "      decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "      metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "  score = metric.compute()\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "54QrSxb-1b7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "YYOdbmX-5zkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "rouge_metric = evaluate.load('rouge')\n",
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]"
      ],
      "metadata": {
        "id": "Zo3zccnO52nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size =2, column_text = 'dialogue', column_summary='summary'\n",
        ")\n",
        "\n",
        "rouge_dict = {rn: score[rn] for rn in rouge_names}\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'])\n",
        "\n",
        "# rouge score closer to 1 is a better metric."
      ],
      "metadata": {
        "id": "xSuuTTXw6Ykk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpreting rouge scores\n",
        "\n",
        "- score ~1 : Strong overlap between the generated summary and reference summary --> highly desirables\n",
        "- 0.5 < Score < 0.7 : Indicate moderate overlap --> high possibility of missing some information\n",
        "- score < 0.5: poor match between the generated summary and reference summary."
      ],
      "metadata": {
        "id": "LnWdMJQv8Nzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ],
      "metadata": {
        "id": "LitqJDIZ8QfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## save tokenizer\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "3LYbHkJz71pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading save tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "yo_F6hpm9OjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"lengthy_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n",
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\", tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "##\n",
        "print('--'*50)\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "print('--'*50)\n",
        "print(\"\\n Reference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "print('--'*50)\n",
        "print(\"\\n Model Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "eQtHCIHh9XVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gS5_HR2naBIU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}